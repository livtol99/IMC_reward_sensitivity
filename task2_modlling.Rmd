---
title: "Models"
author: "Liv Tollånes"
date: "2023-02-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## 2. Multilevel modelling of reward anticipation and self reports of pleasure and experience of reward
Q: Does lab-measured reward anticipation (averaged EEfRT scores) predict self-reported (i) "anticipated reward pleasure" (ii) "experienced reward" in the real world (ESM)? 


	- Use the averaged EEfRT scores created in task 1. (Scores from the 2 days(pos and neg) merged together, but with 3 levels, low, med, high probability)
	
	- Use multilevel modelling, with 3 levels of nested data (questionnaires, nested within days, within participants)
	
	- The analysis would be very close to the paper: https://psyarxiv.com/fnhd9/, but with EEfRT performance as the participant-level predictors (low, med, high) instead of the ISI, RPA scores, and ERQ scores. The EEfRT are obviously not day-level predictors, but more like our covariates baseline, ISI  - instead of baseline participant-level ISI scores, RPA scores, and ERQ scores). "Participant" and "Days within participant" are random intercepts.


```{r}
#Loading packages 

pacman::p_load("tidyverse", "ggpubr", "Hmisc", "corrplot", "rstatix", "psych", "nlme", "lme4", "lmerTest", "JWileymisc","multilevelTools", "texreg", "ordinal", "GGally", "rcompanion", "brant")

#ordinal: creating regression models for ordinal data
#rcompanion: test R squared
# brant: test for proportional odds

```

```{r}
#Loading ESM data

# ESM <- read.csv("./data/ESM_reduced.csv")
# ESM_keys <- read.csv("./data/ESM_keys.csv")
# 
# unique(ESM$Actual_ID) # P022 was never there from the beginning
# 
# ### Removing the excluded rows for low participation
# 
# #Are there still 2-s in the column?
# ESM$Excluded.for.Low.Participation..1...no..2...yes. <-  as.factor(ESM$Excluded.for.Low.Participation..1...no..2...yes.)
# is.factor(ESM$Excluded.for.Low.Participation..1...no..2...yes.)
# unique(ESM$Excluded.for.Low.Participation..1...no..2...yes.)
# 
# # keeping an overview of what participants were removed - for matching with the CBM df
# ESM_exluded_low_part <- filter(ESM, Excluded.for.Low.Participation..1...no..2...yes. == "2")
# ESM_exluded_low_part_ID <- unique(ESM_exluded_low_part$Actual_ID) #The IDs removed are P010, P019, P020, P028, P038, P041, P049, P506, P513
# 
# #Removing from main df
# ESM <- filter(ESM, Excluded.for.Low.Participation..1...no..2...yes. != "2")
# unique(ESM$Excluded.for.Low.Participation..1...no..2...yes.)
# 
# length(unique(ESM$Actual_ID)) #There are 67 unique participants in this df
# 
# 
# 
# ##############################
# #CBM data set
# CBM <- read.csv("./data/preprocessed.csv")
# CBM_vars <- read.csv("./data/CBM_variables.csv")
# 
# length(unique(CBM$ID)) #There are 79 unique IDs - 12 more than in the ESM data
# 
# #Replace weird column values with NA
# CBM[CBM  == "#NULL!"] <- NA
# 
# 
# #Selecting only the relevant columns in the CBM data for better overview
# CBM_sub <- CBM %>% select(c(1:7), 
#                           EEfRT_averaged_lowp, 
#                           EEfRT_averaged_medp, 
#                           EEfRT_averaged_highp,
#                           EEfRT_LowProb_DiffScore,
#                           EEfRT_MedProb_DiffScore,
#                           EEfRT_HighProb_DiffScore,
#                           POSCBMTrainingaccuracy,	
#                           POSCBMTestingaccuracy,
#                           NEGCBMTrainingaccuracy,
#                           NEGCBMTestingaccuracy,
#                           RT_POS_TRAINING_IDX,
#                           RT_NEG_TRAINING_IDX,
#                           RT_Diff_Score,
#                           TEPS_ANT, TEPS_CON, MASQ_AA, MASQ_GD, MASQ_AD,  BAS_Drive, BAS_FS,BAS_RR, BIS)
# 
# CBM_sub$exclude <- NULL
# CBM_sub$participant_id <- NULL
# 
# # 16 participants have NAs in their EEfRT columns - due to lacking EEfRT scores
# 
# #Removing the participants not included in the ESM df first to inspect what IDs are left
# CBM_sub <- filter(CBM_sub, ID != "P010")
# CBM_sub <- filter(CBM_sub, ID != "P019")
# CBM_sub <- filter(CBM_sub, ID != "P020")
# CBM_sub <- filter(CBM_sub, ID != "P028")
# CBM_sub <- filter(CBM_sub, ID != "P038")
# CBM_sub <- filter(CBM_sub, ID != "P041")
# CBM_sub <- filter(CBM_sub, ID != "P049")
# CBM_sub <- filter(CBM_sub, ID != "P506")
# CBM_sub <- filter(CBM_sub, ID != "P513")
# 
# length(unique(CBM_sub$ID)) #Length is now 70, but 12 problematic participants are left. Only 4 of those removed were NA-participants
# 
# # Looking into the original df for these participants - what is the issue?
# #The problematic IDs are: P003, P024, P036, P501, P509, P510, P512, P515, P525, P526, P535, P536
# probID <- subset(CBM, ID %in% c("P003", "P024", "P036", "P501", "P509", "P510", "P512", "P515", "P525", "P526", "P535", "P536"))
# 
# 
# #Further inspection shows that the problematic IDs are caused by NAs in the various prob categories of the EEfRT measurements - so incomplete CBM data. Removing all of them
# CBM_sub <- CBM_sub %>% filter(!ID %in% c("P003", "P024", "P036", "P501", "P509", "P510","P512", "P515", "P525", "P526", "P535", "P536")) #The resulting df 
# 
# 
# #Unique IDs in the subsetted CBM data
# length(unique(CBM_sub$ID)) #The resulting df now has 58 unique participants. There were 67 in the ESM data. 
# CBM_sub_unique_IDs <- unique(CBM_sub$ID)
# 
# #Looking at which IDS in the ESM df do not occur in the CBM df
# unique(ESM$Actual_ID)
# 
# unique(CBM_sub$ID)
# 
# #The IDs that occur in the ESM data but not in the CBM data are:
# #P024, P036, P501, P509, P512, P525, P526, P535, P536 (9 in total)
# #Removing them from the ESM data:
# ESM <- ESM %>% filter(!Actual_ID %in% c("P003","P024", "P036", "P501", "P509", "P512", "P525", "P526", "P535", "P536"))
# 
# unique(ESM$Actual_ID)
# 
# unique(CBM_sub$ID)
# 
# #P022 is in CBM sub - but was never there for ESM. Removing P022 from CBM_sub
# CBM_sub <- CBM_sub %>% filter(!ID %in% "P022")
# 
# length(unique(ESM$Actual_ID))
# length(unique(CBM_sub$ID)) #The two dfs are now equal 
# 
# 


##### Instead of all of the above, I guess an alternative option was to just merge the dfs, and write na.action "na-omit" in the model .....
```



```{r}
# #The next task is to add three EEfRT columns to the ESM data frame, replicating their scores across each row every partipant has
# #Rename Actual_ID in ESM df to ID
# ESM <- ESM %>% rename(ID = Actual_ID)
# 
# #Creating a merged column of subject and day
# ESM$subj_day <- paste(ESM$ID, ESM$Day)
# 
# #Merging the two dfs
# ESM_merged <- merge(ESM, CBM_sub, all.x=TRUE, by="ID")
# ESM_merged <- as_tibble(ESM_merged)
# 
# #reordering columns for better overview
# ESM_merged <- ESM_merged  %>% relocate(subj_day, .after=Day)
# ESM_merged <- ESM_merged  %>% relocate(Excluded.for.Low.Participation..1...no..2...yes., .after=BIS)
# ESM_merged <- ESM_merged  %>% relocate(StartDate, .after=Excluded.for.Low.Participation..1...no..2...yes.)
# ESM_merged <- ESM_merged  %>% relocate(Nationality, .after=StartDate)
# 
# 
# mergeddf <- write.csv(ESM_merged, "./data/ESM_CBM_merged")

```

## Actual modelling

Variable types:
- Outcome/dependent variables (ant/con. pleasure) are ordinal (measured on 7 pt. Likert scale)
- Predictors: EEfRT averages are ratios. Between 0 and 1


I should consider centering the outcome variables (ant/con pleasure - see link: https://philippmasur.de/2018/05/23/how-to-center-in-multilevel-models/)

Christine writes that the multilevel analysis should contain:
- checking residuals
- mean-centring some variables
- running models (with and without outliers)
- generating some plots at the end


# Inspecting the distributions of the data (once more :PPP)
```{r}
ESM_merged <- read.csv("./data/ESM_CBM_merged")

#Distribution of experienced and anticipated pleasure scores
hist(ESM_merged$AntPleasure, col = 'coral2')
hist(ESM_merged$ConPleasure, col = 'steelblue')


#Looking into exact summaries per pleasure score 
# The total distributions are very even across the two measures
Ant_pl <- ESM_merged %>%
  group_by(AntPleasure) %>%
  summarise(counts = n())

Con_pl <- ESM_merged %>%
  group_by(ConPleasure) %>%
  summarise(counts = n())


Ant_pl
Con_pl


#Distribution of proportions according to reward probability 
hist(ESM_merged$EEfRT_averaged_lowp, col = 'coral2') # When the probability of reward is low, the general tendency is for people to not make very much effort (low proportions of people chose the hard task)
hist(ESM_merged$EEfRT_averaged_medp, col = 'green') # When the probability of reward is medium, there is a tendency that 50/50 chooses either the hard or the low task
hist(ESM_merged$EEfRT_averaged_highp, col = 'blue') #When the probability of reward is high, there is a tendency that most people choose the hard task


#Relationship between the two outcome variables - Are they correlated? (Colour by ID or Day)
ggplot(ESM_merged, aes(x=AntPleasure, y=ConPleasure, colour = ID)) + geom_point() + geom_jitter() #Inspection of the plot shows us that the two outcome measures most likely are correlated, with the spread of scores also being equal to each side from the midline. Seems to be relatively even across participants and days


#Are the predictors correlated?
ggplot(ESM_merged, aes(x=EEfRT_averaged_medp, y=EEfRT_averaged_lowp, colour = ID)) + geom_point()
ggplot(ESM_merged, aes(x=EEfRT_averaged_medp, y=EEfRT_averaged_highp, colour = ID)) + geom_point() 
ggplot(ESM_merged, aes(x=EEfRT_averaged_lowp, y=EEfRT_averaged_highp, colour = ID)) + geom_point() 

ggplot(ESM_merged, aes(EEfRT_averaged_medp, EEfRT_averaged_lowp)) + 
  geom_point(color='blue') +
  geom_smooth(method = "lm", colour = 'red')


ggplot(ESM_merged, aes(EEfRT_averaged_medp, EEfRT_averaged_highp)) + 
  geom_point(color='blue') +
  geom_smooth(method = "lm", colour = 'red')

ggplot(ESM_merged, aes(EEfRT_averaged_highp, EEfRT_averaged_lowp)) + 
  geom_point(color='blue') +
  geom_smooth(method = "lm", colour = 'red')

# #From long to wide data
library(tidyr)
wide <- ESM_merged %>% pivot_longer(cols=c('EEfRT_averaged_lowp', 'EEfRT_averaged_medp', 'EEfRT_averaged_highp'),
                    names_to='prob_cond',
                    values_to='Hard_task_proportion')


#Add new column with dummy variables for the probability condition
# Create dummy variables for 2 columns
wide$dummy_prob_group <- NA
wide$dummy_prob_group[wide$prob_cond == "EEfRT_averaged_lowp"] <- 1
wide$dummy_prob_group[wide$prob_cond == "EEfRT_averaged_medp"] <- 2
wide$dummy_prob_group[wide$prob_cond == "EEfRT_averaged_highp"] <- 3

unique(wide$dummy_prob_group)


# Create a line plot with all conditions in one - to see pattern differences
ggplot(wide, aes(x = dummy_prob_group, y = Hard_task_proportion, color = ID)) + geom_line() +
  scale_x_continuous(breaks = seq(1, 3, by = 1))


# Inspect the distribution of anticipated pleasure and experienced pleasure across days for each participant
ggplot(ESM_merged, aes(x = AntPleasure, color = ID)) + geom_jitter(stat="count") +
  scale_x_continuous(breaks = seq(1, 7, by = 1)) + facet_wrap(~Day) + labs(title = "Anticipated Pleasure",
       subtitle = "(Visualised per day of measurement across IDs)", y = "Count", x = "Anticipated Pleasure - 7 pt. Likert Scale") + 
  theme_bw() + theme(legend.position="none")


ggplot(ESM_merged, aes(x = ConPleasure, color = ID)) + geom_jitter(stat="count") +
  scale_x_continuous(breaks = seq(1, 7, by = 1)) + facet_wrap(~Day) + labs(title = "Experienced Pleasure",
       subtitle = "(Visualised per day of measurement)",
       y = "Count", x = "Anticipated Pleasure - 7 pt. Likert Scale") + theme_bw() + theme(legend.position="none")

#These plot shows an indication that the response pattern does not vary much between days. The same response pattern is found between the two pleasure measures, with a tendency for all participants to have more responses in the positive end of the Likert-scale. (5-7)


#Centering and scaling the two outcome variables
#Adding a z-score column to the data frame for the pleasure scores (scaling)
ESM_merged$ConPleasure_Z <- scale(ESM_merged$ConPleasure, center = T, scale = T)
ESM_merged$AntPleasure_Z <- scale(ESM_merged$AntPleasure, center = T, scale = T)

#Mean-Centering the outcome variables
ESM_merged$ConPleasure_centered <- scale(ESM_merged$ConPleasure, center = T, scale = F)
ESM_merged$AntPleasure_centered <- scale(ESM_merged$AntPleasure, center = T, scale = F)

#Visualising the z-scored and centered outcome variables as distributions per participant

ggplot(ESM_merged, aes(ConPleasure)) + geom_density(aes(x=ConPleasure, y=..density.., color = ID), bins=50) + facet_wrap(~ID) + theme(legend.position="none") + ggtitle("Experienced Pleasure - Uncentered") + scale_x_continuous(breaks = seq(1, 7, by = 1))

ggplot(ESM_merged, aes(ConPleasure_Z)) + geom_density(aes(x=ConPleasure_Z, y=..density.., color = ID), bins=50) + facet_wrap(~ID) + theme(legend.position="none") + ggtitle("Experienced Pleasure - Z-scored") 

ggplot(ESM_merged, aes(ConPleasure_centered)) + geom_density(aes(x=ConPleasure_centered, y=..density.., color = ID), bins=50) + facet_wrap(~ID) + theme(legend.position="none")+ ggtitle("Experienced Pleasure - Population Mean Centered") 

```


```{r}
#Inspecting range of EEfRT ratios (predictors)
min(ESM_merged$EEfRT_averaged_lowp) #0
max(ESM_merged$EEfRT_averaged_lowp) #0.81

min(ESM_merged$EEfRT_averaged_medp) #0
max(ESM_merged$EEfRT_averaged_medp) #0.975

min(ESM_merged$EEfRT_averaged_highp) #0.03
max(ESM_merged$EEfRT_averaged_highp) #0.1


#Centering??
# I do not think centering the predictors is necessary - there is a true meaningful 0 (our predictors are ratio variables)
# #Centering predictors - tetsing the same models but with centered predictors
# If scale = TRUE, then z-scores are computed
ESM_merged$EEfRT_averaged_lowp_cen <- scale(ESM_merged$EEfRT_averaged_lowp, center = TRUE, scale = FALSE)
ESM_merged$EEfRT_averaged_medp_cen <- scale(ESM_merged$EEfRT_averaged_medp, center = TRUE, scale = FALSE)
ESM_merged$EEfRT_averaged_highp_cen <- scale(ESM_merged$EEfRT_averaged_highp, center = TRUE, scale = FALSE)

#Testing cumulative link mixed models (haven't checked assumptions yet)



```

#Modelling

# Logistic regression 
A logistic regression is used to classify/predict a categorical (usually binary) outcome  based on previous observations of a data set. Independent variables are used to predict the occurrence or failure of specific events. Standard Logistic regression’s output lies between 0 and 1.

- Logistic regression seeks to study and examine the probabilities of an event's occurrence.
- In our case, the outcome variable is not binary, and so a standard logistic regression is not suitable. 
- Calculates coefficients through maximum likelihood estimation


The fact that I am asked to answer whether the lab measures of reward anticipation can predict the ESM measures of reward anticipation seems kind of strange, when really the power point seems to suggest that the question of interest really regards wether or not the two are associated with each other. 


## Cumulative link mixed model (CLMMs)
- This is a logistic regression model - and the task is becoming a classification task rather than a prediction task. 
- The CLMM allows for analysing ordinal response variables while taking use of random effects as well
- These models are also known as ordered regression models, and proportional odds models
- Estimation is done via maximum likelihood
- Models are fitted with the Laplace approximation and adaptive Gauss.Hermite quadrature
- we're working with cumulative probabilities
- Cumulative probability (or odss, depending on specifications) of falling within or below any of the certain crdered categories in our outcome variable

Goal:
- Determine the realtionship between an ordered factor dependent variable and a set of predictors
- Ideal for analysing Likert-scale data

 
#Regarding scaling variables
I don't think scaling the predictors would be necessary, considering that all three predictors contain the same  format of information. There is a clear meaning of 0 in our case (for the predictors that is) - 0 is a complete absence of hard tasks for the given probbability

#LaPlace approximation
Laplace's approximation provides an analytical expression for a posterior probability distribution by fitting a Gaussian distribution with a mean equal to the MAP solution and a variance equal to the observed Fisher information.

```{r}

#Making sure the outcome variable are ordered factors
ESM_merged$ConPleasure <- factor(ESM_merged$ConPleasure, ordered = TRUE)
ESM_merged$AntPleasure <- factor(ESM_merged$AntPleasure, ordered = TRUE)

########################## Null models #################
#Fitting the null models
ant_null <- clmm(AntPleasure ~ 1 + (1|ID) + (1|subj_day), data = ESM_merged, na.action = "na.omit", Hess = TRUE, link = "logit")

con_null <- clmm(ConPleasure ~ 1 + (1|ID) + (1|subj_day), data = ESM_merged, na.action = "na.omit", Hess = TRUE, link = 'logit')


################### full models ###########

#Random intercept models
antmod1 <- clmm(AntPleasure ~ EEfRT_averaged_lowp + EEfRT_averaged_medp + EEfRT_averaged_highp + (1|ID) + (1|subj_day), data = ESM_merged, na.action = "na.omit", Hess = TRUE, link = 'logit') #logit link gives us the proportional odds mixed model

conmod1 <- clmm(ConPleasure ~ EEfRT_averaged_lowp + EEfRT_averaged_medp + EEfRT_averaged_highp + (1|ID) + (1|subj_day), data = ESM_merged, na.action = "na.omit", Hess = TRUE, link = 'logit')


# Comparing against the null
anova(ant_null, antmod) # the full model does not seem to be much better, no significant difference
anova(con_null, conmod) # the full model does not seem to be much better, no significant difference

#getting the pseudo r squared
#nagelkerke(fit = antmod1, null = ant_null)
 
summary(antmod1)
summary(conmod1)

#Confidence intervals
confint(antmod1) #Both models include 0 in the confidence intervals
confint(conmod1)


#Questions to ask:
# How much variance is explained by the random predictors?
# The primary result is the coefficient table with parameter estimates

#Intuition between interpretation of the clmm() output. Positive coefficients indicate that higher averaged proportions of hard tasks in the given probability category is associated with higher ratings of the outcome measures
```


#Interpreting the model outputs

#Result 
## There does not seem to be any significant associations between our predictors (ratio of hard tasks chosen in the three probability conditions) and reward/pleasure anticipation, nor between the predictors and experienced rewards/pleasure (at p>.05). 

- Ordinal regression outputs indicate the likelihood/odds/proportional odds (depending on the link function applied) of moving from one category to the next at any level in the ordinal structure of the dependent variable (this is why the assumpyion of parallel lines is so important). From lower to higher. 

- There are pretty large standard errors for all estimates - large uncertainty
We can exponentiate the model outputs to get odds ratios



#Trends
Anticipated pleasure (disregarding bad model fit and insignificance):
Low p: The positive estimate indicates that, as the proportion of hard tasks chosen for low probability goes up, people are more likely to have higher self reported anticipation of pleasure

Medium and high p: negative association means that as the proportion of hard tasks chosen increases, people are less likely to report higher anticipated pleasure. (Counterintuitive according to our assumptions. We would assume a positive relationship between choosing harder tasks and reporting higher levels of anticipation)

Experienced pleasure (disregarding bad model fit and insignificance):


#Model fit:
- There are large standard deviations in the estimates
- Confidence intervals all include 0 for the coefficients of both models (no sifgnificance)
- No difference between the null and the full model (random intercept model)
- Assumptions of parrallel lines violated for both models (the effects of the predictors on the ordinal outcome are not the same for each change point in the dependent variable), expect for EEfRT_lowp on the anticipated pleasure

- There seems to be quite some group level variance for ID in both models. Less for days within subjects. (spread in population distribution)
- - Standard errors are also quite wide for all fixed predictors in both, but lowest for "lowp" in both models (sample variation from true population mean)






# Testing model assumptions following model fit
The assumptions of ordered logistic regression must be assessed to ensure the models fitted are indeed valid. 
The assumptions should be tested in the following order:

1. The dependent variable is ordered
2. One or more of the independent variables are either continuous, categorical or ordinal.
3. No multicollinearity between the independent variables
4. Proportional odds



Comment on multicollinearity:
Multicollinearity exists whenever an independent variable is highly correlated with one or more of the other independent variables in a multiple regression equation. Multicollinearity is a problem because it undermines the statistical significance of an independent variable. Other things being equal, the larger the standard error of a regression coefficient, the less likely it is that this coefficient will be statistically significant.

Comment on proportional odds
- This assumption means that the relationship between each pair of outcome groups has to be the same. 
- We want to test the assumption that the effect of each intependent variable is the same for each change point in the dependent variable. So the effect of EEfRT_low_p on going from 1 to 2 on the likert scale should be the same as the effect of that variable on going from 2 to 3 etc. 
- If the relationship between all pairs of groups is the same, then there is only one set of coefficient, which means that there is only one model.
- If this assumption is violated, different models are needed to describe the relationship between each pair of outcome groups.


# 1. The dependent variable is ordered (satisfied)
This assumption is fulfilled - Likert-scale data is our outcome variable. 

# 2. One or more of the predictors are either continuous, categorical, or ordinal (satosfied)
- We're dealing with ratio scale variables. These are considered continuous within their limitation.
- maybe double check this with Christine. Otherwise, consider scaling the predictors


# 3. No multicollinearity between the independent variables (saitisfied)
```{r}
# pacman::p_load("GGally")

#Create numeric outcome columns
ESM_merged$con_num <- as.numeric(ESM_merged$ConPleasure)
ESM_merged$ant_num <- as.numeric(ESM_merged$AntPleasure)

# correlation plot - Testing collinearity
EEfRT_avs <- ESM_merged[, c(11:13)]
ggpairs(EEfRT_avs, title = "Correlation Plot between each predictor Variable")
# No correlation coefficients above 0.8, so there is reason to think there is no multicollinearity. 


```

# 4. Proportional Odds (not satisfied ?? )
Brant test can be used to test this assumption

- The assumption is not violated if the p-values for Brant's test are above .05. 
- So we want the brant test to be non-significant

- we do the test for the entire model, as well as for each independent variable


Findings:

The assumption of proportional odds is not satisfied  according to the brant test for all predictors byt the low probability category (anticipated pleasure)

For experienced pleasure - none of the predictors satisfies the assumption. As such, this model is likely not very good - something is off with its specification. 

```{r}
# We can't do the brant test on the clmm model - we must use the polr() from MASS 
library("MASS")

#Polr does not take the random effects - I found a source stating that they did not matter for the proportional odds assumption, and so a simple polr could be fitted (no random effects) for the brant test. However - not sure :////

ant_polr <- polr(AntPleasure ~ EEfRT_averaged_lowp + EEfRT_averaged_medp + EEfRT_averaged_highp, data = ESM_merged, na.action = "na.omit", Hess = TRUE) 

brant(ant_polr)


  con_polr <- polr(ConPleasure ~ EEfRT_averaged_lowp + EEfRT_averaged_medp + EEfRT_averaged_highp, data = ESM_merged, na.action = "na.omit", Hess = TRUE)

brant(con_polr)



# The assumption of proportional odds is not satisfied  according to the brant test for all predictors byt the low probability category (anticipated pleasure)

# For experienced pleasure - none of the predictors satisfies the assumption 

```



#Interpreting the model output
https://marissabarlaz.github.io/portfolio/ols/ 

Firstly: Difference between probability and odds
- Probabilities are considered proportions or percentages, defined by dividing the occurrences of an event divided by the total number of observations. 
- Odds are the ratio of the probability of one event to the probability of another event, which can be simplified as the ratio of the frequency of X to the frequency of Y.


If the odds equal 1, the probabilities of the outcomes are equal. If the odds are lower than 1, the probability of the second event is greater than the first (aka, if m/p < 1, then P is more likely than M). If odds is higher than 1, the probability of the first event is greater than the second event (if m/p > 1, then M more likely than P).

Log odds are logarithmically transformed odds.

An odds ratio is the ratio of two odds. It tells you if the odds for a particular event is more or less likely in a particular scenario over another.

A log odds ratio is the log of the odds ratio. If a log odds ratio is positive, the specified level boosts the chances of a selected outcome. If a log odds ratio is negative, the specified level decreases the chances of a selected outcome. Log odds are centered around 0 (because ln(1) = 0, so when odds are equal, ln(odds) = 0.

In order to convert from log odds ratios to odds ratios, use exp(X). To convert from log odds ratios to probabilities, use the following formula: probability = exp(X)/(1 + exp(X)). You can also use the plogis() function to do this conversion.














