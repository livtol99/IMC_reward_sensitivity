---
title: "Models"
author: "Liv Toll√•nes"
date: "2023-02-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## 2. Multilevel modelling of reward anticipation and self reports of pleasure and experience of reward
Q: Does lab-measured reward anticipation (averaged EEfRT scores) predict self-reported (i) "anticipated reward pleasure" (ii) "experienced reward" in the real world (ESM)? 


```{r}
#Loading packages 

pacman::p_load("tidyverse", "ggpubr", "Hmisc", "corrplot", "rstatix", "psych", "nlme", "lme4", "lmerTest", "JWileymisc","multilevelTools", "texreg", "ordinal", "GGally", "rcompanion", "brant")

#ordinal: creating regression models for ordinal data
#rcompanion: test R squared
# brant: test for proportional odds

```

```{r}
#Loading ESM data

# ESM <- read.csv("./data/ESM_reduced.csv")
# ESM_keys <- read.csv("./data/ESM_keys.csv")
# 
# unique(ESM$Actual_ID) # P022 was never there from the beginning
# 
# ### Removing the excluded rows for low participation
# 
# #Are there still 2-s in the column?
# ESM$Excluded.for.Low.Participation..1...no..2...yes. <-  as.factor(ESM$Excluded.for.Low.Participation..1...no..2...yes.)
# is.factor(ESM$Excluded.for.Low.Participation..1...no..2...yes.)
# unique(ESM$Excluded.for.Low.Participation..1...no..2...yes.)
# 
# #keeping an overview of what participants were removed - for matching with the CBM df
# ESM_exluded_low_part <- filter(ESM, Excluded.for.Low.Participation..1...no..2...yes. == "2")
# ESM_exluded_low_part_ID <- unique(ESM_exluded_low_part$Actual_ID) #The IDs removed are P010, P019, P020, P028, P038, P041, P049, P506, P513
# 
# #Removing from main df
# ESM <- filter(ESM, Excluded.for.Low.Participation..1...no..2...yes. != "2")
# unique(ESM$Excluded.for.Low.Participation..1...no..2...yes.)
# 
# length(unique(ESM$Actual_ID)) #There are 67 unique participants in this df
# 
# 
# 
# ##############################
# #CBM data set
# CBM <- read.csv("./data/preprocessed.csv")
# CBM_vars <- read.csv("./data/CBM_variables.csv")
# 
# length(unique(CBM$ID)) #There are 79 unique IDs - 12 more than in the ESM data
# 
# #Replace weird column values with NA
# CBM[CBM  == "#NULL!"] <- NA
# 
# 
# #Selecting only the relevant columns in the CBM data for better overview
# CBM_sub <- CBM %>% select(c(1:7),
#                           EEfRT_averaged_lowp,
#                           EEfRT_averaged_medp,
#                           EEfRT_averaged_highp,
#                           EEfRT_LowProb_DiffScore,
#                           EEfRT_MedProb_DiffScore,
#                           EEfRT_HighProb_DiffScore,
#                           POSCBMTrainingaccuracy,
#                           POSCBMTestingaccuracy,
#                           NEGCBMTrainingaccuracy,
#                           NEGCBMTestingaccuracy,
#                           RT_POS_TRAINING_IDX,
#                           RT_NEG_TRAINING_IDX,
#                           RT_Diff_Score,
#                           TEPS_ANT, TEPS_CON, MASQ_AA, MASQ_GD, MASQ_AD,  BAS_Drive, BAS_FS,BAS_RR, BIS)
# 
# CBM_sub$exclude <- NULL
# CBM_sub$participant_id <- NULL
# 
# # 16 participants have NAs in their EEfRT columns - due to lacking EEfRT scores
# 
#  #Removing the participants not included in the ESM df first to inspect what IDs are left
# CBM_sub <- filter(CBM_sub, ID != "P010")
# CBM_sub <- filter(CBM_sub, ID != "P019")
# CBM_sub <- filter(CBM_sub, ID != "P020")
# CBM_sub <- filter(CBM_sub, ID != "P028")
# CBM_sub <- filter(CBM_sub, ID != "P038")
# CBM_sub <- filter(CBM_sub, ID != "P041")
# CBM_sub <- filter(CBM_sub, ID != "P049")
# CBM_sub <- filter(CBM_sub, ID != "P506")
# CBM_sub <- filter(CBM_sub, ID != "P513")
# 
# length(unique(CBM_sub$ID)) #Length is now 70, but 12 problematic participants are left. Only 4 of those removed were NA-participants
# 
# # Looking into the original df for these participants - what is the issue?
# #The problematic IDs are: P003, P024, P036, P501, P509, P510, P512, P515, P525, P526, P535, P536
# probID <- subset(CBM, ID %in% c("P003", "P024", "P036", "P501", "P509", "P510", "P512", "P515", "P525", "P526", "P535", "P536"))
# 
# 
# #Further inspection shows that the problematic IDs are caused by NAs in the various prob categories of the EEfRT measurements - so incomplete CBM data. Removing all of them
# CBM_sub <- CBM_sub %>% filter(!ID %in% c("P003", "P024", "P036", "P501", "P509", "P510","P512", "P515", "P525", "P526", "P535", "P536")) #The resulting df
# 
# #Unique IDs in the subsetted CBM data
# length(unique(CBM_sub$ID)) #The resulting df now has 58 unique participants. There were 67 in the ESM data.
# CBM_sub_unique_IDs <- unique(CBM_sub$ID)
# 
# #Looking at which IDS in the ESM df do not occur in the CBM df
# unique(ESM$Actual_ID)
# 
# unique(CBM_sub$ID)
# 
# #The IDs that occur in the ESM data but not in the CBM data are:
# #P024, P036, P501, P509, P512, P525, P526, P535, P536 (9 in total)
# 
# #Removing them from the ESM data:
# ESM <- ESM %>% filter(!Actual_ID %in% c("P003","P024", "P036", "P501", "P509", "P512", "P525", "P526", "P535", "P536"))
# 
# unique(ESM$Actual_ID)
# 
# unique(CBM_sub$ID)
# 
# #P022 is in CBM sub - but was never there for ESM. Removing P022 from CBM_sub
# CBM_sub <- CBM_sub %>% filter(!ID %in% "P022")
# 
# length(unique(ESM$Actual_ID))
# length(unique(CBM_sub$ID)) #The two dfs are now equal


```



```{r}
# #The next task is to add three EEfRT columns to the ESM data frame, replicating their scores across each row every partipant has
# #Rename Actual_ID in ESM df to ID
# ESM <- ESM %>% rename(ID = Actual_ID)
# 
# #Creating a merged column of subject and day
# ESM$subj_day <- paste(ESM$ID, ESM$Day)
# 
# #Merging the two dfs
# ESM_merged <- merge(ESM, CBM_sub, all.x=TRUE, by="ID")
# ESM_merged <- as_tibble(ESM_merged)
# 
# #reordering columns for better overview
# ESM_merged <- ESM_merged  %>% relocate(subj_day, .after=Day)
# ESM_merged <- ESM_merged  %>% relocate(Excluded.for.Low.Participation..1...no..2...yes., .after=BIS)
# ESM_merged <- ESM_merged  %>% relocate(StartDate, .after=Excluded.for.Low.Participation..1...no..2...yes.)
# ESM_merged <- ESM_merged  %>% relocate(Nationality, .after=StartDate)
# 
# 
# mergeddf <- write.csv(ESM_merged, "./data/ESM_CBM_merged")

```



Variable types:
- Outcome/dependent variables (ant/con. pleasure) are ordinal (measured on 7 pt. Likert scale)
- Predictors: EEfRT averages are ratios. Between 0 and 1


# Inspecting the distributions of the data 

## Outcome measures
```{r}
ESM_merged <- read.csv("./data/ESM_CBM_merged")

#1. Distribution of experienced and anticipated pleasure scores - We observe that the two outcome variables have similar distribution across Likert-options
hist(ESM_merged$AntPleasure, col = 'coral2')
hist(ESM_merged$ConPleasure, col = 'steelblue')


#Looking into exact summaries per pleasure score 
# The total distributions are very even across the two measures
Ant_pl <- ESM_merged %>%
  group_by(AntPleasure) %>%
  summarise(counts = n())

Con_pl <- ESM_merged %>%
  group_by(ConPleasure) %>%
  summarise(counts = n())


Ant_pl
Con_pl


#Relationship between the two outcome variables - Are they correlated? (Colour by ID or Day)
#Inspection of the plot shows us that the two outcome measures most likely are correlated, with the spread of scores also being equal to each side from the midline. Seems to be relatively even across participants and days
ggplot(ESM_merged, aes(x=AntPleasure, y=ConPleasure, colour = ID)) + geom_point() + geom_jitter() + ggtitle("Relation between the two outcome variables")

```

We see that the responses for the two outcome variables, anticipated and experienced pleasure, most likely are correlated. There is a linear relationship with similar spread to each side. We also observe that there is a heavier weight of responses in the higher response categories. 



## Predictors
```{r}
#Distribution of proportions according to reward probability 
hist(ESM_merged$EEfRT_averaged_lowp, col = 'coral2') # When the probability of reward is low, the general tendency is for people to not make very much effort (low proportions of people chose the hard task)
hist(ESM_merged$EEfRT_averaged_medp, col = 'green') # When the probability of reward is medium, there is a tendency that 50/50 chooses either the hard or the low task
hist(ESM_merged$EEfRT_averaged_highp, col = 'blue') #When the probability of reward is high, there is a tendency that most people choose the hard task



#Are the predictors correlated?
 ggplot(ESM_merged, aes(x=EEfRT_averaged_medp, y=EEfRT_averaged_lowp, colour = ID)) + geom_point() + geom_smooth(method = "lm", colour = 'red')
ggplot(ESM_merged, aes(x=EEfRT_averaged_medp, y=EEfRT_averaged_highp, colour = ID)) + geom_point() + geom_smooth(method = "lm", colour = 'red') # Strongest relationship between medium and ghigh
ggplot(ESM_merged, aes(x=EEfRT_averaged_lowp, y=EEfRT_averaged_highp, colour = ID)) + geom_point() + geom_smooth(method = "lm", colour = 'red')

```

## Comments on the plots above 
1. Proportions according to probability category
- (Red histogram plot) When the probability of reward is low, the general tendency is for people to not make very much effort. this is reflected in the left skewed plot. 
- (Green histogram plot) When the probability of reward is medium, there is a tendency that 50/50 chooses either the hard or the low task. This is reflected in the almost bell shaped curve, centered at around .5. 
- (Blue Histogram Plot): When the probability of reward is high, there is a tendency that most people choose the hard task - as reflected in the right skewed graph


2. Correlation of predictors
The three plots confirms what was found in the correlation analysis - there seems to be some degree of positive correlations between the low and medium probability categhory, and between the medium and high probability categories. High and low do not seem to be correlated. 


# Individual trajectories - Distribution of the proportion of hard tasks choses in the three probability categories

```{r}
# #From long to wide data
library(tidyr)
wide <- ESM_merged %>% pivot_longer(cols=c('EEfRT_averaged_lowp', 'EEfRT_averaged_medp', 'EEfRT_averaged_highp'),
                    names_to='prob_cond',
                    values_to='Hard_task_proportion')


#Add new column with dummy variables for the probability condition
# Create dummy variables for 2 columns
wide$dummy_prob_group <- NA
wide$dummy_prob_group[wide$prob_cond == "EEfRT_averaged_lowp"] <- 1
wide$dummy_prob_group[wide$prob_cond == "EEfRT_averaged_medp"] <- 2
wide$dummy_prob_group[wide$prob_cond == "EEfRT_averaged_highp"] <- 3

# unique(wide$dummy_prob_group) We have the correct dummy variables


# Create a line plot with all conditions in one - to see pattern differences. lines represent individual trajectories
ggplot(wide, aes(x = dummy_prob_group, y = Hard_task_proportion, color = ID)) + geom_line() +
  scale_x_continuous(breaks = seq(1, 3, by = 1)) + xlab("Probability group") + ggtitle("Proportion of hard tasks chosen in the three probability categories") + theme(legend.position="none")


```
We see that the proprotion of hard tasks chosen generally increase as the probability of reward increases


# Distributions across Likert options and days
```{r}
# Inspect the distribution of anticipated pleasure and experienced pleasure across days for each participant
ggplot(ESM_merged, aes(x = AntPleasure, color = ID)) + geom_jitter(stat="count") +
  scale_x_continuous(breaks = seq(1, 7, by = 1)) + facet_wrap(~Day) + labs(title = "Anticipated Pleasure",
       subtitle = "(Visualised per day of measurement across IDs)", y = "Count", x = "Anticipated Pleasure - 7 pt. Likert Scale") + 
  theme_bw() + theme(legend.position="none")


ggplot(ESM_merged, aes(x = ConPleasure, color = ID)) + geom_jitter(stat="count") +
  scale_x_continuous(breaks = seq(1, 7, by = 1)) + facet_wrap(~Day) + labs(title = "Experienced Pleasure",
       subtitle = "(Visualised per day of measurement)",
       y = "Count", x = "Experienced Pleasure - 7 pt. Likert Scale") + theme_bw() + theme(legend.position="none")


#Centering and scaling the two outcome variables
#Adding a z-score column to the data frame for the pleasure scores (scaling)
ESM_merged$ConPleasure_Z <- scale(ESM_merged$ConPleasure, center = T, scale = T)
ESM_merged$AntPleasure_Z <- scale(ESM_merged$AntPleasure, center = T, scale = T)

#Mean-Centering the outcome variables
ESM_merged$ConPleasure_centered <- scale(ESM_merged$ConPleasure, center = T, scale = F)
ESM_merged$AntPleasure_centered <- scale(ESM_merged$AntPleasure, center = T, scale = F)

#Visualising the z-scored and centered outcome variables as distributions per participant

# Uncentered
ggplot(ESM_merged, aes(ConPleasure)) + geom_density(aes(x=ConPleasure, y=..density.., color = ID), bins=50) + facet_wrap(~ID) + theme(legend.position="none") + ggtitle("Experienced Pleasure - Uncentered") + scale_x_continuous(breaks = seq(1, 7, by = 1))

ggplot(ESM_merged, aes(AntPleasure)) + geom_density(aes(x=AntPleasure, y=..density.., color = ID), bins=50) + facet_wrap(~ID) + theme(legend.position="none") + ggtitle("Anticipated Pleasure - Uncentered") + scale_x_continuous(breaks = seq(1, 7, by = 1))


#Z-scored
ggplot(ESM_merged, aes(ConPleasure_Z)) + geom_density(aes(x=ConPleasure_Z, y=..density.., color = ID), bins=50) + facet_wrap(~ID) + theme(legend.position="none") + ggtitle("Experienced Pleasure - Z-scored") 

ggplot(ESM_merged, aes(AntPleasure_Z)) + geom_density(aes(x=AntPleasure_Z, y=..density.., color = ID), bins=50) + facet_wrap(~ID) + theme(legend.position="none") + ggtitle("Anticipated Pleasure - Z-scored") 


#Mean centered
ggplot(ESM_merged, aes(ConPleasure_centered)) + geom_density(aes(x=ConPleasure_centered, y=..density.., color = ID), bins=50) + facet_wrap(~ID) + theme(legend.position="none")+ ggtitle("Experienced Pleasure - Population Mean Centered") 

ggplot(ESM_merged, aes(AntPleasure_centered)) + geom_density(aes(x=AntPleasure_centered, y=..density.., color = ID), bins=50) + facet_wrap(~ID) + theme(legend.position="none")+ ggtitle("Anticipated Pleasure - Population Mean Centered") 

```
The two first plots indicate that the response pattern does not vary much between days. The same response pattern is found between the two pleasure measures, with a tendency for all participants to have more responses in the positive end of the Likert-scale. (5-7)

The next three groups of plots give us more detailed information regarding individual distributions of scores throughout the week of measurements. (NB. The uncentered plot is likely to be more useful, as the scale simply represents the Likert options). Despite different scales on the y-axis for anticipated and ecperienced pleasure (probably due to differences in amounts of responses), there seems to be very similar patterns/curvature of individual distributions for the two mesures. 



#Inspection of ranges and centering variables

```{r}
#Inspecting range of EEfRT ratios (predictors)
min(ESM_merged$EEfRT_averaged_lowp) #0
max(ESM_merged$EEfRT_averaged_lowp) #0.81

min(ESM_merged$EEfRT_averaged_medp) #0
max(ESM_merged$EEfRT_averaged_medp) #0.975

min(ESM_merged$EEfRT_averaged_highp) #0.03
max(ESM_merged$EEfRT_averaged_highp) #0.1


#Centering
# I do not think centering the predictors is necessary - there is a true meaningful 0 (our predictors are ratio variables)
# #Centering predictors - tetsing the same models but with centered predictors
# If scale = TRUE, then z-scores are computed
ESM_merged$EEfRT_averaged_lowp_cen <- scale(ESM_merged$EEfRT_averaged_lowp, center = TRUE, scale = FALSE)
ESM_merged$EEfRT_averaged_medp_cen <- scale(ESM_merged$EEfRT_averaged_medp, center = TRUE, scale = FALSE)
ESM_merged$EEfRT_averaged_highp_cen <- scale(ESM_merged$EEfRT_averaged_highp, center = TRUE, scale = FALSE)

```

#Modelling

# Logistic regression 
- Logistic regression seeks to study and examine the probabilities of an event's occurrence.
- In our case, the outcome variable is not binary, and so a standard logistic regression is not suitable. A cumulative link mixed model is more appropriate. 


#Regarding scaling variables
I don't think scaling the predictors would be necessary, considering that all three predictors contain the same  format of information. There is a clear meaning of 0 in our case (for the predictors that is) - 0 is a complete absence of hard tasks for the given probbability

```{r}

#Making sure the outcome variable are ordered factors
ESM_merged$ConPleasure <- factor(ESM_merged$ConPleasure, ordered = TRUE)
ESM_merged$AntPleasure <- factor(ESM_merged$AntPleasure, ordered = TRUE)

########################## Null models #################
#Fitting the null models
ant_null <- clmm(AntPleasure ~ 1 + (1|ID) + (1|subj_day), data = ESM_merged, na.action = "na.omit", Hess = TRUE, link = "logit")

con_null <- clmm(ConPleasure ~ 1 + (1|ID) + (1|subj_day), data = ESM_merged, na.action = "na.omit", Hess = TRUE, link = 'logit')


################### full models ###########

#Random intercept models
antmod1 <- clmm(AntPleasure ~ EEfRT_averaged_lowp + EEfRT_averaged_medp + EEfRT_averaged_highp + (1|ID) + (1|subj_day), data = ESM_merged, na.action = "na.omit", Hess = TRUE, link = 'logit') #logit link gives us the proportional odds mixed model

conmod1 <- clmm(ConPleasure ~ EEfRT_averaged_lowp + EEfRT_averaged_medp + EEfRT_averaged_highp + (1|ID) + (1|subj_day), data = ESM_merged, na.action = "na.omit", Hess = TRUE, link = 'logit')


# Comparing against the null
anova(ant_null, antmod1) # the full model does not seem to be much better, no significant difference
anova(con_null, conmod1) # the full model does not seem to be much better, no significant difference


summary(antmod1)
summary(conmod1)

#Confidence intervals
confint(antmod1) #Both models include 0 in the confidence intervals
confint(conmod1)

```


#Interpreting the model outputs

#Result 
## There does not seem to be any significant associations between our predictors (ratio of hard tasks chosen in the three probability conditions) and reward/pleasure anticipation, nor between the predictors and experienced rewards/pleasure (at p>.05). 

- There are pretty large standard errors for all estimates - large uncertainty


#Model fit:
- There are large standard deviations in the estimates
- Confidence intervals all include 0 for the coefficients of both models (no sifgnificance)
- No difference between the null and the full model (random intercept model)
- Assumptions of parallel lines violated for both models (the effects of the predictors on the ordinal outcome are not the same for each change point in the dependent variable), expect for EEfRT_lowp on the anticipated pleasure

- There seems to be quite some group level variance for ID in both models. Less for days within subjects. (spread in population distribution)
- Standard errors are also quite wide for all fixed predictors in both, but lowest for "lowp" in both models (sample variation from true population mean)


# Testing model assumptions following model fit
The assumptions of ordered logistic regression must be assessed to ensure the models fitted are indeed valid. 
The assumptions should be tested in the following order:

1. The dependent variable is ordered
2. One or more of the independent variables are either continuous, categorical or ordinal.
3. No multicollinearity between the independent variables
4. Proportional odds


# 1. The dependent variable is ordered (satisfied)
This assumption is fulfilled - Likert-scale data is our outcome variable. 

# 2. One or more of the predictors are either continuous, categorical, or ordinal (satisfied)
- We're dealing with ratio scale variables. These are considered continuous within their limitation.
- maybe double check this with Christine. Otherwise, consider scaling the predictors


# 3. No multicollinearity between the independent variables (satisfied)
```{r}
# pacman::p_load("GGally")

#Create numeric outcome columns
ESM_merged$con_num <- as.numeric(ESM_merged$ConPleasure)
ESM_merged$ant_num <- as.numeric(ESM_merged$AntPleasure)

# correlation plot - Testing collinearity
EEfRT_avs <- ESM_merged[, c(11:13)]
ggpairs(EEfRT_avs, title = "Correlation Plot between each predictor Variable")
# No correlation coefficients above 0.8, so there is reason to think there is no multicollinearity. 


```

# 4. Proportional Odds (not satisfied - unless there is a more correct way to asess this assumption for mixed models)
Brant test can be used to test this assumption

- The assumption is not violated if the p-values for Brant's test are above .05. 
- So we want the brant test to be non-significant

- we do the test for the entire model, as well as for each independent variable


Findings:

The assumption of proportional odds is not satisfied  according to the brant test for all predictors byt the low probability category (anticipated pleasure)

For experienced pleasure - none of the predictors satisfies the assumption. As such, this model is likely not very good - something is off with its specification. 

```{r}
# We can't do the brant test on the clmm model - we must use the polr() from MASS 
library("MASS")

#Polr does not take the random effects - I found a source stating that they did not matter for the proportional odds assumption, and so a simple polr could be fitted (no random effects) for the brant test. However - not sure :////

ant_polr <- polr(AntPleasure ~ EEfRT_averaged_lowp + EEfRT_averaged_medp + EEfRT_averaged_highp, data = ESM_merged, na.action = "na.omit", Hess = TRUE) 

brant(ant_polr)


con_polr <- polr(ConPleasure ~ EEfRT_averaged_lowp + EEfRT_averaged_medp + EEfRT_averaged_highp, data = ESM_merged, na.action = "na.omit", Hess = TRUE)

brant(con_polr)



# The assumption of proportional odds is not satisfied  according to the brant test for all predictors byt the low probability category (anticipated pleasure)

#For experienced pleasure - none of the predictors satisfies the assumption 

```


# Concluding remarks
No signififcant relationships were found between the three probabiliy categories and the two outcome measures (experienced and anticipated pleasure). However, the assumption of parallel lines is seemingly violated, suggesting that a different model might be more appropriate. 












