---
title: "task3_corrected_day_of_week"
author: "Liv Toll√•nes"
date: "2023-07-20"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#Loading packages
pacman::p_load("tidyverse", "ggpubr","dplyr", "lme4", "lmerTest", "nlme", "flexmix")
```


```{r}
#Loading ESM data

#Since I want to correct for the day of the week, I load the full data set in this case
ESM_full <- read.csv(("./data/ESM_full.csv"))
ESM_keys <- read.csv("./data/ESM_keys.csv")

#Selecting only the relevant columns
ESM<- ESM_full[, c("Actual_ID", "Day", "Date", "Excluded.for.Low.Participation..1...no..2...yes.", "ConPleasure", "AntPleasure", "PA", "NA.")]


##### Removing the rows excluded for low participation (as marked in the column by the same name)

#Are there any 2-s in the column? - yes there are
ESM$Excluded.for.Low.Participation..1...no..2...yes. <-  as.factor(ESM$Excluded.for.Low.Participation..1...no..2...yes.)
is.factor(ESM$Excluded.for.Low.Participation..1...no..2...yes.)
unique(ESM$Excluded.for.Low.Participation..1...no..2...yes.)

# keeping an overview of what participants were removed - for matching with the CBM df
ESM_exluded_low_part <- filter(ESM, Excluded.for.Low.Participation..1...no..2...yes. == "2")
ESM_exluded_low_part_ID <- unique(ESM_exluded_low_part$Actual_ID) #The IDs removed are P010, P019, P020, P028, P038, P041, P049, P506, P513

#Now, actually removing them from main df
ESM <- filter(ESM, Excluded.for.Low.Participation..1...no..2...yes. != "2")
unique(ESM$Excluded.for.Low.Participation..1...no..2...yes.)

length(unique(ESM$Actual_ID)) #There are 67 unique participants in this df

#Dropping the Exclusion column to keep only relevant columns
ESM$Excluded.for.Low.Participation..1...no..2...yes. <- NULL

#Rename Actual_ID in ESM df to ID
ESM_3 <- ESM %>% rename(ID = Actual_ID)

```



# Creating column for day of the week
We want to correct for day of the week furthwr down in the analysis. So we need to add the days first

```{r}
# Lubridate has date related functions
pacman::p_load("lubridate")

#Change date format from character to dates
ESM_3$Date <- as.Date(ESM_3$Date, format = "%d/%m/%Y")

#Adding the days of week in string format
ESM_3$day_of_week <- weekdays(ESM_3$Date)

#Numerical format
ESM_3$day_of_week_num <- wday(ESM_3$Date, week_start = 1)
```



```{r}
# Loading CBM data
CBM <- read.csv("./data/preprocessed.csv")
CBM_vars <- read.csv("./data/CBM_variables.csv")


length(unique(CBM$ID)) #There are 79 unique IDs - 12 more than in the ESM data

#Replace weird column values with NA
CBM[CBM  == "#NULL!"] <- NA

#Selecting only the relevant columns in the CBM data for better overview
CBM_3 <- CBM %>% dplyr::select(ID,RT_Diff_Score)

#Remove in the CBM df those removed in the ESM data due to low participation
# these were P010, P019, P020, P028, P038, P041, P049, P506, P513
CBM_3 <- filter(CBM_3, ID != "P010")
CBM_3 <- filter(CBM_3, ID != "P019")
CBM_3 <- filter(CBM_3, ID != "P020")
CBM_3 <- filter(CBM_3, ID != "P028")
CBM_3 <- filter(CBM_3, ID != "P038")
CBM_3 <- filter(CBM_3, ID != "P041")
CBM_3 <- filter(CBM_3, ID != "P049")
CBM_3 <- filter(CBM_3, ID != "P506")
CBM_3 <- filter(CBM_3, ID != "P513")


length(unique(CBM_3$ID))
length(unique(ESM_3$ID)) #There are 3 less participants in the ESM data now

uniqueCBM <- unique(CBM_3$ID)
uniqueESM <- unique(ESM_3$ID)

#What IDs do not occur in the ESM data? Remove these from the CBM data
uniqueCBM
uniqueESM #P022, P510, P515 do not exist in the ESM data. Remove them

CBM_3 <- filter(CBM_3, ID != "P022")
CBM_3 <- filter(CBM_3, ID != "P510")
CBM_3 <- filter(CBM_3, ID != "P515")

length(unique(CBM_3$ID))
length(unique(ESM_3$ID)) #Now, they are the same length. Merge them for further work

df_task3 <- merge(CBM_3, ESM_3, all.x=TRUE, by="ID")
df_task3 <- df_task3  %>% relocate(Day, .after=ID)

# A strange value appears for participant P503 on day 2. Set this value to NA - and then omit NAs
#df_task3[df_task3 == "#DIV/0!"] <- NA

# By inspection, we see that there has been an issue in the coding of the "Day" variable in the original ESM data frame. 
# P037: day 4 is both coded for 20/06 and 21/06
# P054: day 2 is coded for both 25/06 and 26/06
# P504: day 6 is both coded for 29/06 and 30/06

#Fixing the coding of the days
# Recode specific values for the Day column for IDs P037 and P504

# For ID P037, recode day 4 on 21/06/2019 to 5
df_task3$Day <- with(df_task3, ifelse(ID == "P037" & Day == 4 & Date == "2019-06-21", 5, Day))

# For ID P504, recode day 2 on 26/06/2019 to 3 and day 6 on 30/06/2019 to 7
df_task3$Day <- with(df_task3, ifelse(ID == "P504" & Day == 2 & Date == "2019-06-26", 3, Day))
df_task3$Day <- with(df_task3, ifelse(ID == "P504" & Day == 6 & Date == "2019-06-30", 7, Day))


#Creating a merged column of subject and day
df_task3$subj_day <- paste(df_task3$ID, df_task3$Day)

any(is.na(df_task3)) #contains NAs - keeping them so far
df_task3[!complete.cases(df_task3), ]
```
#Create SD scores for positive and negative emotions and replicate per day 
```{r}
# Create SD and duplicate for all rows on multiple columns

SD_pos <- df_task3 %>%
  dplyr::group_by(subj_day) %>%
  summarise(SD_PA = sd(PA))


SD_neg <- df_task3 %>%
  dplyr::group_by(subj_day) %>%
  summarise(SD_NA = sd(NA.))

df_task3 <- merge(df_task3, SD_pos, all.x=TRUE, by="subj_day")
df_task3 <- merge(df_task3, SD_neg, all.x=TRUE, by="subj_day")



# Choose only the relevant columns
df3_modelling <- df_task3[, c("subj_day", "ID", "Day","Date", "day_of_week", "day_of_week_num", "RT_Diff_Score", "SD_PA", "SD_NA")]


#Since we're working with aggregated values (SD scores per day), we only need only one row per participant
#Removing duplicate rows
df3_modelling  <- df3_modelling [!duplicated(df3_modelling ), ]

#Do any of the IDs now contain more rows?
which(duplicated(df3_modelling$subj_day)) #now, we only have one row per subject per day


# The resulting df contains quite some NAs. This is a result of an inability to calculate deviation scores per day due the fact that some participants only have one NA and PA score for certain days

#The NAs will simply be ignored in the modelling, but it might be a sign that daily variation in affect scores is not a good separation of the data to use as predictors in the modelling process

```

# Modelling

## What variables do I have?
For both models, the outcome (SD-scores) are continuous. Therefore --> linear mixed models. (Assuming there is a linear relation between the DV and the IV)


#Linear regression:
- LinReg uses a linear equation to identify the line of best fit - and thereby enables prediction of the output of the dependent variable based on the independent ones. 
- Describes a linear relationship between variables
- Calculates coefficients/estimates best fitting regression line through ordinary least squares

- Here, we ask: how is CBM adaptability linearly related to daily mood lability?


#Sanity checks - is there anay relation between the variables at all?


```{r}
sapply(df3_modelling, class)
```


```{r}

# 1. Variation in affect across days
ggplot(df3_modelling, aes(day_of_week_num, SD_NA, color= ID)) + 
  geom_point() +
  geom_smooth(method = "lm", colour = 'blue') + geom_line()+ ggtitle("Relation between variation in negative affect and day of the week")

ggplot(df3_modelling, aes(day_of_week_num, SD_PA,  color= ID)) + 
  geom_point() +
  geom_smooth(method = "lm", colour = 'blue') + geom_line()+ ggtitle("Relation between variation in positive affect and day of the week")




#2. Negative emotion lability and CBM adaptability
ggplot(df3_modelling, aes(RT_Diff_Score, SD_PA, col = ID)) + 
  geom_point() +
  geom_smooth(method = NULL, colour = 'red') + ggtitle("Relation between positive emotion lability and CBM adaptability")


ggplot(df3_modelling, aes(RT_Diff_Score, SD_NA, col = ID)) + 
  geom_point() +
  geom_smooth(method = NULL, colour = 'red') + ggtitle("Relation between negative emotion lability and CBM adaptability")



```


# Comments on data visualisations - does correcting for day of the week matter?
As was the case for the previous analysis (not correcting for day of the week), a linear model will still likely not be a good choice if we want to investigate the relationship between daily mood lability and CBM adaptability. 

1. There is still a seemingly random pattern in daily mood lability across days for individuals when corrected for day of the week.  However, if one really wishes to pick out some kind of trend, there is a very slight tendency for a larger deviation in negative affect at the end of the week - we observe a very slight increase at the end of the week (see the plot called "Relation between variation in negative affect and day of the week.") There is also a very weak tendency for a higher variation in positive affect at the beginning of the week than at the end of the week (see plot called "Relation between variation in positive affect and day of the week")

2. When allowing for a different fitted line than a linear one,is seems like there is not a linear relationship between daily emotion lability and the recognition difference score per participant. Again, suggesting that a linear model is a poor choice for this data

Still relevant: Another noteworthy point is that the SD measure per day is likely not a good measure for emotion lability - due to lacking data points, some subjects don't have a score each day. This is because certain subjects only had one data point regarding PA or NA. on that given day


# Z-scoring variables
```{r}
# scale = F only centers the variables, but does not compute Z-scores
df3_modelling$RT_Diff_Score_cen <- scale(df3_modelling$RT_Diff_Score, scale = F)
df3_modelling$SD_PA_cen <- scale(df3_modelling$SD_PA, scale = F )
df3_modelling$SD_NA_cen <- scale(df3_modelling$SD_NA, scale = F)
```

#Fitting the models

In the following section, I am fitting the same models as originally, but reinterpreting the results based on the new "Day"-measure (so day of the week instead of day after the participant started the study )

- Linear mixed effects models


#### Refit the models and reinterpret. Then send mail to Katherine about the new results


```{r}
## Positive - uncentered

#Remove Nas before modelling to perform model comparison
df3_modelling <- na.omit(df3_modelling)


#Positive affect
#Null models
#random intercept for ID and day of the week , no fixed effects but with overall intercept
mpos0 <- lmer(SD_PA ~ 1 + (1|ID) + (1|day_of_week_num), data = df3_modelling, REML = T, # Restricted maximum likelihood, small sample size
             na.action = "na.omit")

#Random intercept for ID and, no fixed effects but with overall intercept
mpos02 <- lmer(SD_PA ~ 1 + (1|ID), data = df3_modelling, REML = T,na.action = "na.omit")


#Fuller models
#With variability for ID and day of the week
mpos <- lmer(SD_PA ~ RT_Diff_Score + (1|ID) + (1|day_of_week_num) , data = df3_modelling, REML = T, na.action = "na.omit")

#With variability for ID o
mpos2 <- lmer(SD_PA ~ RT_Diff_Score + (1|ID), data = df3_modelling, REML = T, na.action = "na.omit")

#Comparison
#Are the models statistically significant fdrom each other?
anova(mpos0, mpos02) #no statistically significant difference
anova(mpos0, mpos) # no statistically significant difference
anova(mpos0, mpos2) # seem to be different
anova(mpos02, mpos) # no statistically significant difference
anova(mpos02, mpos2) #no difference
anova(mpos2, mpos) #no difference

#Which model is the best?
# Calculate BIC values for each model
bic_values <- c(
  BIC(mpos0),
  BIC(mpos02),
  BIC(mpos),
  BIC(mpos2)
)

# Create a data frame with model names and BIC values
model_names <- c("mpos0", "mpos02", "mpos", "mpos2")
bic_table <- data.frame(Model = model_names, BIC = bic_values)

# Print the table
print(bic_table)


# mpos02 is the best based on BIC values - indicating that the RT_Diff_Score is not a good predictor of mood lability across days
```



```{r}

#Fitting the same models, only centered
#Null models
#random intercept for ID and day of the week , no fixed effects but with overall intercept
mpos0cen <- lmer(SD_PA_cen ~ 1 + (1|ID) + (1|day_of_week_num), data = df3_modelling, REML = T, # Restricted maximum likelihood, small sample size
             na.action = "na.omit")

#Random intercept for ID, no fixed effects but with overall intercept
mpos02cen <- lmer(SD_PA_cen ~ 1 + (1|ID), data = df3_modelling, REML = T, # Restricted maximum likelihood, small sample size
             na.action = "na.omit")


#Fuller models
#With variability for ID and day of the week
mposcen <- lmer(SD_PA_cen ~ RT_Diff_Score_cen + (1|ID) + (1|day_of_week_num) , data = df3_modelling, REML = T, na.action = "na.omit")

#With variability for ID
mpos2cen <- lmer(SD_PA_cen ~ RT_Diff_Score_cen + (1|ID) , data = df3_modelling, REML = T, na.action = "na.omit")


#Which model is the best?
# Calculate BIC values for each model
bic_values <- c(
  BIC(mpos0cen),
  BIC(mpos02cen),
  BIC(mposcen),
  BIC(mpos2cen)
)

# Create a data frame with model names and BIC values
model_names <- c("mpos0cen", "mpos02cen", "mposcen", "mpos2cen")
bic_table <- data.frame(Model = model_names, BIC = bic_values)

# Print the table
print(bic_table)


# mpos02 is the best based on BIC values - indicating that the RT_Diff_Score is not a good predictor of mood lability across days


```


```{r}

#Negative affect
#Null models

#random intercept for ID and day of the week , no fixed effects but with overall intercept
mneg0 <- lmer(SD_NA ~ 1 + (1|ID) + (1|day_of_week_num), data = df3_modelling, REML = T, # Restricted maximum likelihood, small sample size
             na.action = "na.omit")

#Random intercept for ID, no fixed effects but with overall intercept
mneg02 <- lmer(SD_NA ~ 1 + (1|ID), data = df3_modelling, REML = T, # Restricted maximum likelihood, small sample size
             na.action = "na.omit")


#Fuller models
#With variability for ID and day of the week
mneg <- lmer(SD_NA ~ RT_Diff_Score + (1|ID) + (1|day_of_week_num), data = df3_modelling, REML = T, na.action = "na.omit") # Restricted maximum likelihood, small sample size
             
#With variability for ID 
mneg2 <- lmer(SD_NA ~ RT_Diff_Score + (1|ID) , data = df3_modelling, REML = T,
             na.action = "na.omit")





#Comparison
#Are the models statistically significant fdrom each other?
anova(mneg0, mneg02) #no statistically significant difference
anova(mneg0, mneg) # no statistically significant difference
anova(mneg0, mneg2) # seem to be different
anova(mneg02, mneg) # no statistically significant difference
anova(mneg02, mneg2) #no difference
anova(mneg2, mneg) #no difference


#Fitting the same models, only centered
#Null models
mneg0cen <- lmer(SD_NA_cen ~ 1 + (1|ID) + (1|day_of_week_num), data = df3_modelling, REML = T, # Restricted maximum likelihood, small sample size
             na.action = "na.omit")

mneg02cen <- lmer(SD_NA_cen ~ 1 + (1|ID), data = df3_modelling, REML = T, # Restricted maximum likelihood, small sample size
             na.action = "na.omit")

#Fuller models
mnegcen <- lmer(SD_NA_cen ~ RT_Diff_Score_cen + (1|ID) + (1|day_of_week_num) , data = df3_modelling, REML = T,
             na.action = "na.omit")

mneg2cen <- lmer(SD_NA_cen ~ RT_Diff_Score_cen + (1|ID) , data = df3_modelling, REML = T,
             na.action = "na.omit")


#Are the models statistically significant fdrom each other?
anova(mneg0cen, mneg02cen) #no statistically significant difference
anova(mneg0cen, mnegcen) # no statistically significant difference
anova(mneg0cen, mneg2cen) # seem to be different
anova(mneg02cen, mnegcen) # no statistically significant difference
anova(mneg02cen, mneg2cen) #no difference
anova(mneg2cen, mnegcen) #no difference



#Which model is the best?
# Calculate BIC values for each model
bic_values <- c(
  BIC(mneg0cen),
  BIC(mneg02cen),
  BIC(mnegcen),
  BIC(mneg2cen)
)

# Create a data frame with model names and BIC values
model_names <- c("mneg0cen", "mneg02cen", "mnegcen", "mneg2cen")
bic_table <- data.frame(Model = model_names, BIC = bic_values)

# Print the table
print(bic_table) #mneg02cen seems to be the best one


# Conclusion so far: a linear mixed effects model is no good way of modelling the relationship between difference scores and mood lability scores. No relationships are found at a significant level

```



# Concluding remarks - did correcting for day of the week make a differece?
- Correcting for day of the week did not produce any overall differences from the original analyses, and the suggestions in "task3_models.RMD" still hold




