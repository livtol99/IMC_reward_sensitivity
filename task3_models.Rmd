---
title: "task3_models"
author: "Liv Toll√•nes"
date: "2023-02-23"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#Loading packages
pacman::p_load("tidyverse", "ggpubr","dplyr", "lme4", "lmerTest", "nlme", "flexmix")
```

```{r}
#Loading ESM data

ESM <- read.csv("./data/ESM_reduced.csv")
ESM_keys <- read.csv("./data/ESM_keys.csv")

#unique(ESM$Actual_ID) # P022 was never there from the beginning



### Removing the rows excluded for low participation (as marked in the column by the same name)
#Are there any 2-s in the column? - yes there are
ESM$Excluded.for.Low.Participation..1...no..2...yes. <-  as.factor(ESM$Excluded.for.Low.Participation..1...no..2...yes.)
is.factor(ESM$Excluded.for.Low.Participation..1...no..2...yes.)
unique(ESM$Excluded.for.Low.Participation..1...no..2...yes.)

# keeping an overview of what participants were removed - for matching with the CBM df
ESM_exluded_low_part <- filter(ESM, Excluded.for.Low.Participation..1...no..2...yes. == "2")
ESM_exluded_low_part_ID <- unique(ESM_exluded_low_part$Actual_ID) #The IDs removed are P010, P019, P020, P028, P038, P041, P049, P506, P513

#Now, actually removing them from main df
ESM <- filter(ESM, Excluded.for.Low.Participation..1...no..2...yes. != "2")
unique(ESM$Excluded.for.Low.Participation..1...no..2...yes.)

length(unique(ESM$Actual_ID)) #There are 67 unique participants in this df

#Selecting only the columns relevant for this analysis (Actual_ID, Day, PA, NA.)

ESM_3 <- ESM %>% dplyr::select(c(1:2), PA, NA.)

#Rename Actual_ID in ESM df to ID
ESM_3 <- ESM_3 %>% rename(ID = Actual_ID)

```


```{r}
# Loading CBM data
CBM <- read.csv("./data/preprocessed.csv")
CBM_vars <- read.csv("./data/CBM_variables.csv")


length(unique(CBM$ID)) #There are 79 unique IDs - 12 more than in the ESM data

#Replace weird column values with NA
CBM[CBM  == "#NULL!"] <- NA

#Selecting only the relevant columns in the CBM data for better overview
CBM_3 <- CBM %>% dplyr::select(ID,RT_Diff_Score)

#Remove in the CBM df those removed in the ESM data due to low participation
# these were P010, P019, P020, P028, P038, P041, P049, P506, P513
CBM_3 <- filter(CBM_3, ID != "P010")
CBM_3 <- filter(CBM_3, ID != "P019")
CBM_3 <- filter(CBM_3, ID != "P020")
CBM_3 <- filter(CBM_3, ID != "P028")
CBM_3 <- filter(CBM_3, ID != "P038")
CBM_3 <- filter(CBM_3, ID != "P041")
CBM_3 <- filter(CBM_3, ID != "P049")
CBM_3 <- filter(CBM_3, ID != "P506")
CBM_3 <- filter(CBM_3, ID != "P513")


length(unique(CBM_3$ID))
length(unique(ESM_3$ID)) #There are 3 less participants in the ESM data now

uniqueCBM <- unique(CBM_3$ID)
uniqueESM <- unique(ESM_3$ID)

#What IDs do not occur in the ESM data? Remove these from the CBM data
uniqueCBM
uniqueESM #P022, P510, P515 do not exist in the ESM data. Remove them

CBM_3 <- filter(CBM_3, ID != "P022")
CBM_3 <- filter(CBM_3, ID != "P510")
CBM_3 <- filter(CBM_3, ID != "P515")

length(unique(CBM_3$ID))
length(unique(ESM_3$ID)) #Now, they are the same length. Merge them for further work

df_task3 <- merge(CBM_3, ESM_3, all.x=TRUE, by="ID")
df_task3 <- df_task3  %>% relocate(Day, .after=ID)

# A strange value appears for participant P503 on day 2. Set this value to NA - and then omit NAs
df_task3[df_task3 == "#DIV/0!"] <- NA



#Code for the correction of an error detected in "task3_dayofweek_corrected.Rmd.". There seems to be an issue witrh how "Day" was originally coded. This became apparent when adding dates from the full data set. Should be corrected before the rest of this markdown is run.

# By inspection, we see that there has been an issue in the coding of the "Day" variable in the original ESM data frame. 
# P037: day 4 is both coded for 20/06 and 21/06
# P054: day 2 is coded for both 25/06 and 26/06
# P504: day 6 is both coded for 29/06 and 30/06

#Fixing the coding of the days
# Recode specific values for the Day column for IDs P037 and P504

# For ID P037, recode day 4 on 21/06/2019 to 5
df_task3$Day <- with(df_task3, ifelse(ID == "P037" & Day == 4 & Date == "2019-06-21", 5, Day))

# For ID P504, recode day 2 on 26/06/2019 to 3 and day 6 on 30/06/2019 to 7
df_task3$Day <- with(df_task3, ifelse(ID == "P504" & Day == 2 & Date == "2019-06-26", 3, Day))
df_task3$Day <- with(df_task3, ifelse(ID == "P504" & Day == 6 & Date == "2019-06-30", 7, Day))


#Creating a merged column of subject and day
df_task3$subj_day <- paste(df_task3$ID, df_task3$Day)

any(is.na(df_task3)) #contains NAs - keeping them so far
df_task3[!complete.cases(df_task3), ]

```

#Create SD scores for positive and negative emotions and replicate per day 
```{r}
# Create SD and duplicate for all rows on multiple columns

SD_pos <- df_task3 %>%
  dplyr::group_by(subj_day) %>%
  summarise(SD_PA = sd(PA))


SD_neg <- df_task3 %>%
  dplyr::group_by(subj_day) %>%
  summarise(SD_NA = sd(NA.))

df_task3 <- merge(df_task3, SD_pos, all.x=TRUE, by="subj_day")
df_task3 <- merge(df_task3, SD_neg, all.x=TRUE, by="subj_day")

# We actually only need one row per participant for the analysis, since we're using variables with only one measurement per ID
# Select subj_day,ID, Day,RT_Diff_Score, SD_PA, and SD_NA - with only one row per participant 

df3_modelling <- df_task3 %>% select(c(1:2), RT_Diff_Score, SD_PA, SD_NA, Day)

#Removing duplicate rows
df3_modelling  <- df3_modelling [!duplicated(df3_modelling ), ]

#Do any of the IDs now contain more rows?
which(duplicated(df3_modelling$subj_day)) #Now, we have no duplicated rows for subj_day. (Thus, all individuals have only one row per day)

# The resulting df contains quite some NAs. This is a result of an inability to calculate deviation scores per day due the fact that some participants only have one NA and PA score for certain days

#The NAs will simply be ignored in the modelling, but it might be a sign that daily variation in affect scores is not a good separation of the data to use as predictors in the modelling process

```

# Modelling

## What variables do I have?
For both models, the outcome (SD-scores) are continuous. Therefore --> linear mixed models. (Assuming there is a linear relation between the DV and the IV)


#Linear regression:
- LinReg uses a linear equation to identify the line of best fit - and thereby enables prediction of the output of the dependent variable based on the independent ones. 
- Describes a linear relationship between variables
- Calculates coefficients/estimates best fitting regression line through ordinary least squares

- Here, we ask: how is CBM adaptability linearly related to daily mood lability?


#Sanity checks - is there anay relation between the variables at all?

```{r}

# 1. Variation in affect across days
ggplot(df3_modelling, aes(Day, SD_NA, color= ID)) + 
  geom_point() +
  geom_smooth(method = "lm", colour = 'blue') + geom_line()+ ggtitle("Relation between variation in negative affect and days")

ggplot(df3_modelling, aes(Day, SD_PA,  color= ID)) + 
  geom_point() +
  geom_smooth(method = "lm", colour = 'blue') + geom_line()+ ggtitle("Relation between variation in positive affect and days")




#2. Negative emotion lability and CBM adaptability
ggplot(df3_modelling, aes(RT_Diff_Score, SD_PA, col = ID)) + 
  geom_point() +
  geom_smooth(method = NULL, colour = 'red') + ggtitle("Relation between positive emotion lability and CBM adaptability")


ggplot(df3_modelling, aes(RT_Diff_Score, SD_NA, col = ID)) + 
  geom_point() +
  geom_smooth(method = NULL, colour = 'red') + ggtitle("Relation between negative emotion lability and CBM adaptability")



```

# Comments on data visualisations
Based on the visualisations made, there are clear indications that a linear model will not be the proper choice if we want to investigate the relationship between daily mood lability and CBM adaptability. 

1. There is a seemingly random pattern in daily mood lability across days for individuals. See plot called "Relation between variation in negative affect and days" and "Relation between variation in positive affect and days".

2. When allowing for a different fitted line than a linear one, it becomes relatively apparent that there is not a linear relationship between daily emotion lability and the recognition difference score per participant. Again, suggesting that a linear model is a poor choice for this data

Another noteworthy point is that the SD measure per day is likely not a good measure for emotion lability - due to lacking data points, some subjects don't have a score each day. This is because certain subjects only had one data point regarding PA or NA. on that given day



# Z-scoring variables
```{r}
# scale = F only centers the variables, but does not compute Z-scores
df3_modelling$RT_Diff_Score_cen <- scale(df3_modelling$RT_Diff_Score, scale = F)
df3_modelling$SD_PA_cen <- scale(df3_modelling$SD_PA, scale = F )
df3_modelling$SD_NA_cen <- scale(df3_modelling$SD_NA, scale = F)
```

#Fitting the models
The requested model of using subjects nested withing days seems to be an issue - overfits data. Detailed comments follow below the code chunk. 

```{r}
#Doesn't work - see notes under "issue" below. this is because there are more random effects combinations than observations of diff scores. The nesting of days must be specified in a different way

# mpos <- lmer(SD_PA ~ RT_Diff_Score + (1|subj_day), data = df3_modelling, REML = T,# Restricted maximum likelihood, small sample size
#               na.action = "na.omit")


#Remove Nas before modelling to perform model comparison
df3_modelling <- na.omit(df3_modelling)


#Positive affect
mpos0 <- lmer(SD_PA ~ 1 + (1|ID) + (1|Day), data = df3_modelling, REML = T, # Restricted maximum likelihood, small sample size 
             na.action = "na.omit")

mpos02 <- lmer(SD_PA ~ 1 + (1|ID), data = df3_modelling, REML = T, # Restricted maximum likelihood, small sample size 
             na.action = "na.omit")

mpos <- lmer(SD_PA ~ RT_Diff_Score + (1|ID) + (1|Day) , data = df3_modelling, REML = T, 
             na.action = "na.omit")

mpos2 <- lmer(SD_PA ~ RT_Diff_Score + (1|ID) + (1|Day) , data = df3_modelling, REML = T, 
             na.action = "na.omit")


#Fitting the same models, only centered

mpos0cen <- lmer(SD_PA_cen ~ 1 + (1|ID) + (1|Day), data = df3_modelling, REML = T, # Restricted maximum likelihood, small sample size 
             na.action = "na.omit")

mpos02cen <- lmer(SD_PA_cen ~ 1 + (1|ID), data = df3_modelling, REML = T, # Restricted maximum likelihood, small sample size 
             na.action = "na.omit")

mposcen <- lmer(SD_PA_cen ~ RT_Diff_Score_cen + (1|ID) + (1|Day) , data = df3_modelling, REML = T, 
             na.action = "na.omit")

mpos2cen <- lmer(SD_PA_cen ~ RT_Diff_Score_cen + (1|ID) , data = df3_modelling, REML = T, 
             na.action = "na.omit")


anova(mpos0cen, mpos02cen) #no statistically significant difference 
anova(mpos0cen, mposcen) # no statistically significant difference
anova(mpos2cen, mposcen) #no statistically significant difference

BIC(mpos02cen, mpos2cen) #mpos02 seems to be best
BIC(mpos0cen, mpos2cen) # equal
BIC(mpos0cen, mposcen) # mpos0 best
BIC(mpos2cen, mposcen) #less complex model is the best


#Negative affect
# mneg <- lmer(SD_NA ~ RT_Diff_Score + (1|ID) + (1|Day), data = df3_modelling, REML = T, # Restricted maximum likelihood, small sample size 
#              na.action = "na.omit") This model overfits the data. Consider making a less complex model


#Overfits
#mneg0 <- lmer(SD_NA ~ 1 + (1|ID) + (1|Day), data = df3_modelling, REML = T, # Restricted maximum likelihood, small sample size 
             na.action = "na.omit")

mneg02 <- lmer(SD_NA ~ 1 + (1|ID), data = df3_modelling, REML = T, # Restricted maximum likelihood, small sample size 
             na.action = "na.omit")

#Overfits
#mneg <- lmer(SD_NA ~ RT_Diff_Score + (1|ID) + (1|Day) , data = df3_modelling, REML = T, 
             na.action = "na.omit")

mneg2 <- lmer(SD_NA ~ RT_Diff_Score + (1|ID) , data = df3_modelling, REML = T, 
             na.action = "na.omit")


anova(mneg02, mneg2) # No statistically significant difference
summary(mneg2)

#Fitting the same models, only centered

#Overfits
#mneg0cen <- lmer(SD_NA_cen ~ 1 + (1|ID) + (1|Day), data = df3_modelling, REML = T, # Restricted maximum likelihood, small sample size 
             na.action = "na.omit")

mneg02cen <- lmer(SD_NA_cen ~ 1 + (1|ID), data = df3_modelling, REML = T, # Restricted maximum likelihood, small sample size 
             na.action = "na.omit")

#Overfits
#mnegcen <- lmer(SD_NA_cen ~ RT_Diff_Score_cen + (1|ID) + (1|Day) , data = df3_modelling, REML = T, 
             na.action = "na.omit")

mneg2cen <- lmer(SD_NA_cen ~ RT_Diff_Score_cen + (1|ID) , data = df3_modelling, REML = T, 
             na.action = "na.omit")


anova(mneg2cen, mneg02cen) #no statistically significant difference 

summary(mneg2cen)



# Conclusion so far: a linear mixed effects model is no good way of modelling this relationship, especially if we are to take time ("day") into account. However with these models, no relationship is found at a significant level

```
#Issue
- I understand it so that I am asked to use subj_day as random intercepts for this task. WWith the current data structure, I have one observation per unique subj_day value. This is not possible as the combination of random efefcts exceeds the number of observations
- I have tried, but cannot seem to add the nesting of days within participants without overfitting. 
- There might be an issue in trying to predict more values out from singular ones (emotion lability (several scores per p) ~ diff)
- Including random slopes in this situation seems to be too complex to make sense
- For negative emotion lability, Day cannot be incorporated without overfitting. 

# Concluding remarks
- The full models are not significantly different from the null models - (this holds for both positive and negatuve affect). This is suggestive of poor model fit.
- The desired nesting structure is not possible given the structure of the data, as far as I can see. 
- Without having investigated model assumptions any further, the linear multilevel seems to be a poor choice if the desired outcome is to make conclusions regarding the relationship between daily emotion lability and CBM adaptability. A better choice is suggested at the bottom of this markdown.  



#Doing the same models only with a weekly deviance score instead
What would happen if we try to model the outcome again only with weekly emotion lability instead?

```{r}
# Creating weekly SD scores 
df_task3_noday <- df_task3 %>% select(ID, subj_day, RT_Diff_Score, PA, NA.)
df_task3_noday <-  df_task3_noday[!duplicated(df_task3_noday),]

SD_pos_week <- df_task3_noday %>%
  dplyr::group_by(ID) %>%
  summarise(SD_PA_week = sd(PA))


SD_neg_week <- df_task3_noday %>%
  dplyr::group_by(ID) %>%
  summarise(SD_NA_week = sd(NA.))

SD_scores <- merge(SD_pos_week,SD_neg_week, by = "ID")

#Removing PA and NA-scores now that we have calculated a weekly score
df_task3_noday <- df_task3_noday %>% select(ID, RT_Diff_Score)

SD_week <- merge(df_task3_noday, SD_scores, by="ID")


#Removing duplicate rows
SD_week  <- SD_week[!duplicated(SD_week ), ]

#Do any of the IDs now contain more rows?
which(duplicated(SD_week$ID)) #Now, we have no duplicated rows for each ID

# The resulting df contains quite some NAs. This is a result of an inability to calculate deviation scores per day due the fact that some participants only have one NA and PA score for certain days

#The NAs will simply be ignored in the modelling, but it might be a sign that daily variation in affect scores is not a good separation of the data to use as predictors in the modelling process


# Z-scoring the variables (mean 0, sd 1)
SD_week$RT_Diff_Score_z <- scale(SD_week$RT_Diff_Score)
SD_week$SD_PA_week_z <- scale(SD_week$SD_PA_week)
SD_week$SD_NA_week_z <- scale(SD_week$SD_NA_week)


```

# How is the data distributed now?
```{r}

#Negative emotion lability and CBM adaptability
ggplot(SD_week, aes(RT_Diff_Score, SD_NA_week)) +
  geom_line(method = NULL, colour = 'red') + ggtitle("Relation between negative, weekly emotion lability and CBM adaptability")

# Positive
ggplot(SD_week, aes(RT_Diff_Score, SD_PA_week, col = ID)) +
  geom_line(method = "lm", colour = 'blue') + ggtitle("Relation between positive, weekly emotion lability and CBM adaptability")



hist(SD_week$SD_PA_week)
hist(SD_week$SD_NA_week)
hist(SD_week$RT_Diff_Score)


#Relationship between the two outcome variables - Are they correlated? (Colour by ID or Day)
ggplot(SD_week, aes(x=RT_Diff_Score, y=SD_NA_week, colour = ID)) + geom_point() # does not seem to be any relation

ggplot(SD_week, aes(x=RT_Diff_Score, y=SD_PA_week, colour = ID)) + geom_point()


# There seems to not be a linear relationship between weekly mood fluctuations and CBM adaptability
ggplot(SD_week,aes(RT_Diff_Score, SD_NA_week, col = ID)) + 
  geom_point() +
  geom_smooth(method = NULL, colour = 'red') + geom_line()+ ggtitle("Relation between weekly mood fluctuations and Recognition Diff score")





```



# Concluding remarks - Analysis in total
- It does not look like the suggested model is a good way to assess the relationship between individual weekly mood fluctuations and Rec_test poerformance. The relationship between the two variables are not linear.
- I do not think a weekly aggregation of mood lability is going to capture what we really want. Wee loose the temporal aspect of emotion-lability, and thus move away from the desired insights. 
- This analysis pretty much shows that there is no linear relationship between daily emotion lability and CBM adaptability. This does not necessarily mean that there is no relationship at all, simply that the current model framework is not sufficient to capture it. 

# Suggestion
- I suggest to move to a Bayesian framework, which would allow for a better incorporation of each individual's emotion lability across time. 





